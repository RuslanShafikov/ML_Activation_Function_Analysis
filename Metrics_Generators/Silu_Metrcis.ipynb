{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e14325db-6391-4539-b526-027fbef01e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import time\n",
    "import numpy as np\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model_path = \"../ML/LLM-s/test_silu/checkpoint-12500\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path).to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model.eval()\n",
    "\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "prompts = [t for t in dataset[\"text\"][:1000] if len(t.strip()) > 10]\n",
    "\n",
    "# Perplexity measuring\n",
    "ppls = []\n",
    "loss_fct = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "for prompt in prompts:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=128)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "    if input_ids.size(1) < 2:\n",
    "        continue\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        shift_logits = outputs.logits[..., :-1, :].contiguous()\n",
    "        shift_labels = input_ids[..., 1:].contiguous()\n",
    "\n",
    "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                        shift_labels.view(-1))\n",
    "        ppl = torch.exp(loss).item()\n",
    "        ppls.append(ppl)\n",
    "\n",
    "mean_ppl = np.mean(ppls) if ppls else float(\"nan\")\n",
    "\n",
    "# speed measure\n",
    "num_iterations = 100\n",
    "num_repeats = 5\n",
    "times = []\n",
    "\n",
    "inputs = tokenizer(\"This is a speed test.\", return_tensors=\"pt\").to(device)\n",
    "for _ in range(num_repeats):\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_iterations):\n",
    "            outputs = model(**inputs)\n",
    "    end = time.time()\n",
    "    times.append(end - start)\n",
    "\n",
    "mean_time = np.mean(times)\n",
    "std_time = np.std(times)\n",
    "latency = mean_time / num_iterations\n",
    "num_tokens = inputs[\"input_ids\"].numel() * num_iterations\n",
    "tokens_per_sec = num_tokens / mean_time\n",
    "\n",
    "# Results in csv:\n",
    "results = pd.DataFrame([{\n",
    "    \"Model\": model_path,\n",
    "    \"Device\": \"CPU\",\n",
    "    \"Perplexity\": mean_ppl,\n",
    "    \"Inference Time (s)\": mean_time,\n",
    "    \"Std Dev (s)\": std_time,\n",
    "    \"Latency (ms)\": latency * 1000,\n",
    "    \"Throughput (tokens/sec)\": tokens_per_sec\n",
    "}])\n",
    "\n",
    "results.to_csv(\"../ML/Metrics/Silu_metrics_cpu.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928b0e98-cad9-41ff-bbdc-475905c2c190",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (CUDA)",
   "language": "python",
   "name": "python312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
